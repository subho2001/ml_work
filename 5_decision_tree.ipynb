{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv(\"../dataset/play_tennis.csv\")\n",
    "\n",
    "attributes = ['outlook', 'temp', 'humidity', 'wind']\n",
    "target_attribute = 'play'\n",
    "\n",
    "X = dataset[attributes]\n",
    "y = dataset[target_attribute]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_splits = 5\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(labels):\n",
    "    label_counts = {}\n",
    "    for i in labels:\n",
    "        if i in label_counts:\n",
    "            label_counts[i] += 1\n",
    "        else:\n",
    "            label_counts[i] = 1\n",
    "\n",
    "    entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for count in label_counts.values():\n",
    "        pi = count / total_samples\n",
    "        entropy -= pi * math.log2(pi)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(attribute_values, labels):\n",
    "    total_entropy = calculate_entropy(labels)\n",
    "    attribute_value_counts = {}\n",
    "    weighted_entropy = 0\n",
    "\n",
    "    for value in set(attribute_values):\n",
    "        subset_labels = [labels[i] for i in range(len(attribute_values)) if attribute_values[i] == value]\n",
    "        weight = len(subset_labels) / len(labels)\n",
    "        attribute_value_counts[value] = len(subset_labels)\n",
    "        weighted_entropy += weight * calculate_entropy(subset_labels)\n",
    "\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "\n",
    "    return information_gain, attribute_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree(data, target_attribute, attributes):\n",
    "    if len(set(data[target_attribute])) == 1:\n",
    "        return data[target_attribute].iloc[0]\n",
    "\n",
    "    if len(attributes) == 0:\n",
    "        return data[target_attribute].mode().iloc[0]\n",
    "    best_attribute = max(attributes, key=lambda attr: calculate_information_gain(data[attr], data[target_attribute])[0])\n",
    "    attribute_values = set(data[best_attribute])\n",
    "\n",
    "    decision_tree = {best_attribute: {}}\n",
    "    for value in attribute_values:\n",
    "        subset_data = data[data[best_attribute] == value].reset_index(drop=True)\n",
    "        decision_tree[best_attribute][value] = build_decision_tree(subset_data, target_attribute, [attr for attr in attributes if attr != best_attribute])\n",
    "\n",
    "    return decision_tree\n",
    "decision_tree = build_decision_tree(dataset, target_attribute, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, decision_tree):\n",
    "    attribute = list(decision_tree.keys())[0]\n",
    "    value = data[attribute]\n",
    "\n",
    "    if value in decision_tree[attribute]:\n",
    "        prediction = decision_tree[attribute][value]\n",
    "\n",
    "        if isinstance(prediction, dict):\n",
    "            return predict(data, prediction)\n",
    "        else:\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Actual Predicted\n",
      "8    Yes       Yes\n",
      "1     No        No\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def batch_predict(data, decision_tree):\n",
    "    return [predict(sample, decision_tree) for _, sample in data.iterrows()]\n",
    "\n",
    "test_predictions = batch_predict(X_test, decision_tree)\n",
    "\n",
    "# Compare predictions with actual labels in the testing set\n",
    "comparison = pd.DataFrame({'Actual': y_test, 'Predicted': test_predictions})\n",
    "print(comparison)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(comparison['Actual'] == comparison['Predicted']) / len(comparison)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "tp = cm[0,0]\n",
    "fp = cm[0,1]\n",
    "fn = cm[1,0]\n",
    "tn = cm[1,1]\n",
    "accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
