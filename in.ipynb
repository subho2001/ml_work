{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"../dataset/Salary.csv\")\n",
    "X = df[\"Year of Experience\"].values\n",
    "y = df[\"Salary\"].str.replace(',', '').astype(int).values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_splits = 5\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    n=0\n",
    "    d=0\n",
    "    m=0\n",
    "    b=0\n",
    "    for i in range(X.shape[0]):\n",
    "        n = n+ ((X[i]-X.mean()) * (y[i]- y.mean()))\n",
    "        d = d+ (X[i]-X.mean())**2\n",
    "        m = n/d\n",
    "        b = y.mean() - (m*X.mean())\n",
    "        return m,b\n",
    "m,b = linear_regression(X_train, y_train)\n",
    "def predict(X, m, b):\n",
    "    return m*X+b\n",
    "print(\"Slope(m): \", m)\n",
    "print(\"Intercept(b)\", b)\n",
    "print(\"Predict value: \", predict(X_test[0], m, b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataframe = pd.read_csv('Financial_Coverage-2.csv')\n",
    "\n",
    "X = dataframe.iloc[:, :-1]\n",
    "y = dataframe.iloc[:, -1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_splits = 5\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    num_samples, num_features = X.shape\n",
    "    weights = np.zeros(num_features)\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        model = np.dot(X, weights) + bias\n",
    "        predictions = sigmoid(model)\n",
    "\n",
    "        dw = (1 / num_samples) * np.dot(X.T, (predictions - y))\n",
    "        db = (1 / num_samples) * np.sum(predictions - y)\n",
    "\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def predict(X, weights, bias):\n",
    "    model = np.dot(X, weights) + bias\n",
    "    predictions = sigmoid(model)\n",
    "    predictions_cls = [1 if i > 0.5 else 0 for i in predictions]\n",
    "    return predictions_cls\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_iterations = 5000\n",
    "weights, bias = logistic_regression(X_train, y_train, learning_rate, num_iterations)\n",
    "\n",
    "predictions = predict(X_test, weights, bias)\n",
    "accuracy = np.mean(predictions == y_test) * 100\n",
    "print(\"Logistic Regression Accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"DLBCL-2.csv\")\n",
    "\n",
    "X = df.drop(columns='target').values\n",
    "y = df['target'].values\n",
    "\n",
    "n_splits = 5\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Encode the target variable 'y_train' into numerical values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "    # Then use y_train_encoded instead of y_train in the knn_predict function\n",
    "    def knn_predict(X_train, y_train, x_test, k=5):\n",
    "        distances = np.sqrt(np.sum((X_train - x_test) ** 2, axis=1))\n",
    "        indices = np.argsort(distances)[:k]\n",
    "        neighbors = y_train_encoded[indices]\n",
    "        return np.bincount(neighbors).argmax()\n",
    "\n",
    "    y_pred = [knn_predict(X_train, y_train_encoded, x_test) for x_test in X_test]\n",
    "\n",
    "    # Decode integer labels back to string labels\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Print average accuracy across all folds\n",
    "print(\"Average Accuracy: {:.2f}%\".format(np.mean(accuracy_scores) * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv(\"DLBCL-2.csv\")\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:,-1]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the target variable 'y'\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "y_train = pd.Series(y_train) \n",
    "# n_splits = 3\n",
    "# indices = np.arange(len(X))\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "\n",
    "# for i in range(n_splits):\n",
    "#     test_indices = fold_indices[i]\n",
    "#     train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "#     X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "#     y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001,n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X.values):\n",
    "                condition = y.iloc[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2*self.w - np.dot(x_i, y.iloc[idx]))\n",
    "                    self.b -= self.lr * y.iloc[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.dot(X, self.w) + self.b\n",
    "        return np.sign(prediction).astype(int)\n",
    "clf = SVM()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "accuracy=accuracy_score(y_test,predictions)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv(\"../dataset/play_tennis.csv\")\n",
    "\n",
    "attributes = ['outlook', 'temp', 'humidity', 'wind']\n",
    "target_attribute = 'play'\n",
    "\n",
    "X = dataset[attributes]\n",
    "y = dataset[target_attribute]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_splits = 5\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the DLBCL dataset\n",
    "df = pd.read_csv(\"DLBCL-2.csv\")\n",
    "\n",
    "# Define gene expression attributes and target attribute\n",
    "attributes = [col for col in df.columns if col != 'target']\n",
    "\n",
    "# Split features and target variable\n",
    "X = df[attributes]\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "# Split dataset into features (X) and target variable (y)\n",
    "# X = dataset[attributes]\n",
    "# y = dataset[target_attribute]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    label_counts = {}\n",
    "    for i in labels:\n",
    "        if i in label_counts:\n",
    "            label_counts[i] += 1\n",
    "        else:\n",
    "            label_counts[i] = 1\n",
    "\n",
    "    entropy = 0\n",
    "    total_samples = len(labels)\n",
    "    for count in label_counts.values():\n",
    "        pi = count / total_samples\n",
    "        entropy -= pi * math.log2(pi)\n",
    "\n",
    "    return entropy\n",
    "def calculate_information_gain(attribute_values, labels):\n",
    "    total_entropy = calculate_entropy(labels)\n",
    "    attribute_value_counts = {}\n",
    "    weighted_entropy = 0\n",
    "\n",
    "    for value in set(attribute_values):\n",
    "        subset_labels = [labels[i] for i in range(len(attribute_values)) if attribute_values[i] == value]\n",
    "        weight = len(subset_labels) / len(labels)\n",
    "        attribute_value_counts[value] = len(subset_labels)\n",
    "        weighted_entropy += weight * calculate_entropy(subset_labels)\n",
    "\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "\n",
    "    return information_gain, attribute_value_counts\n",
    "\n",
    "def build_decision_tree(data, target_attribute, attributes):\n",
    "    if len(set(data[target_attribute])) == 1:\n",
    "        return data[target_attribute].iloc[0]\n",
    "\n",
    "    if len(attributes) == 0:\n",
    "        return data[target_attribute].mode().iloc[0]\n",
    "    best_attribute = max(attributes, key=lambda attr: calculate_information_gain(data[attr], data[target_attribute])[0])\n",
    "    attribute_values = set(data[best_attribute])\n",
    "\n",
    "    decision_tree = {best_attribute: {}}\n",
    "    for value in attribute_values:\n",
    "        subset_data = data[data[best_attribute] == value].reset_index(drop=True)\n",
    "        decision_tree[best_attribute][value] = build_decision_tree(subset_data, target_attribute, [attr for attr in attributes if attr != best_attribute])\n",
    "\n",
    "    return decision_tree\n",
    "decision_tree = build_decision_tree(dataset, target_attribute, attributes)\n",
    "\n",
    "def predict(data, decision_tree):\n",
    "    attribute = list(decision_tree.keys())[0]\n",
    "    value = data[attribute]\n",
    "\n",
    "    if value in decision_tree[attribute]:\n",
    "        prediction = decision_tree[attribute][value]\n",
    "\n",
    "        if isinstance(prediction, dict):\n",
    "            return predict(data, prediction)\n",
    "        else:\n",
    "            return prediction\n",
    "def batch_predict(data, decision_tree):\n",
    "    return [predict(sample, decision_tree) for _, sample in data.iterrows()]\n",
    "\n",
    "test_predictions = batch_predict(X_test, decision_tree)\n",
    "\n",
    "# Compare predictions with actual labels in the testing set\n",
    "comparison = pd.DataFrame({'Actual': y_test, 'Predicted': test_predictions})\n",
    "# print(comparison)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(comparison['Actual'] == comparison['Predicted']) / len(comparison)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.read_csv('DLBCL-2.csv')\n",
    "\n",
    "def train_naive_bayes(data, target):\n",
    "    class_probabilities = {}\n",
    "    feature_probabilities = {}\n",
    "\n",
    "    total_samples = len(data)\n",
    "\n",
    "    for class_label, class_count in data[target].value_counts().items():\n",
    "        class_probabilities[class_label] = class_count / total_samples\n",
    "        class_data = data[data[target] == class_label]\n",
    "\n",
    "        for feature in data.columns[:-1]: \n",
    "       \n",
    "            feature_counts = class_data[feature].value_counts()\n",
    "\n",
    "            for value, count in feature_counts.items():\n",
    "                feature_probabilities.setdefault(class_label, {}).setdefault(feature, {})[value] = count / class_count\n",
    "\n",
    "    return class_probabilities, feature_probabilities\n",
    "\n",
    "def predict_naive_bayes(sample, class_probabilities, feature_probabilities):\n",
    "    predictions = {}\n",
    "\n",
    "    for class_label, class_probability in class_probabilities.items():\n",
    "        likelihood = 1.0\n",
    "\n",
    "        for feature, value in sample.items():\n",
    "            if feature != target:\n",
    "                likelihood *= feature_probabilities.get(class_label, {}).get(feature, {}).get(value, 0)\n",
    "\n",
    "        predictions[class_label] = class_probability * likelihood\n",
    "\n",
    "    return max(predictions, key=predictions.get)\n",
    "\n",
    "target = 'target'\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "n_splits = 3\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "\n",
    "for i in range(n_splits):\n",
    "    test_indices = fold_indices[i]\n",
    "    train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "class_probabilities, feature_probabilities = train_naive_bayes(pd.concat([X_train, y_train], axis=1), target)\n",
    "\n",
    "y_pred = X_test.apply(lambda sample: predict_naive_bayes(sample, class_probabilities, feature_probabilities), axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "X = df.iloc[:, 1: ]\n",
    "# y = df.iloc[:, -1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "cov_matrix = np.cov(X_scaled, rowvar=False)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "num_components = 2\n",
    "principal_components = eigenvectors[:, :num_components]\n",
    "\n",
    "X_pca =np.dot(X_scaled,principal_components)\n",
    "\n",
    "X_pca = X_scaled.dot(principal_components)\n",
    "\n",
    "print(\"Eigen Values: \")\n",
    "print(eigenvalues)\n",
    "print(\"principal component: \")\n",
    "print(principal_components)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_estimators):\n",
    "            bootstrap_indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_sampled = X.iloc[bootstrap_indices]\n",
    "            y_sampled = y.iloc[bootstrap_indices]\n",
    "\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                random_state=42\n",
    "            )\n",
    "            tree.fit(X_sampled, y_sampled)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.round(np.mean(predictions, axis=0))\n",
    "\n",
    "# Read the dataset\n",
    "# df = pd.read_csv('Financial_Coverage-2.csv')\n",
    "\n",
    "# # Perform one-hot encoding for categorical variables\n",
    "# df_encoded = pd.get_dummies(df)\n",
    "\n",
    "# # Separate features (X) and target variable (y)\n",
    "# X = df_encoded.drop(columns=['smoker_yes'], axis=1)  # Remove the target variable column\n",
    "# y = df_encoded['smoker_yes']  # Target variable\n",
    "\n",
    "# # Perform cross-validation\n",
    "# n_splits = 6\n",
    "# indices = np.arange(len(X))\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# fold_indices = np.array_split(indices, n_splits)\n",
    "\n",
    "# accuracies = []\n",
    "\n",
    "# for i in range(n_splits):\n",
    "#     test_indices = fold_indices[i]\n",
    "#     train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "\n",
    "#     X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "#     y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "#     # Train the random forest classifier\n",
    "#     n_trees = 100\n",
    "#     random_forest = RandomForestClassifier(n_estimators=n_trees)\n",
    "#     random_forest.fit(X_train, y_train)\n",
    "\n",
    "#     # Make predictions\n",
    "#     rf_predictions = random_forest.predict(X_test)\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = accuracy_score(y_test, rf_predictions)\n",
    "#     accuracies.append(accuracy)\n",
    "\n",
    "# # Print the average accuracy over all folds\n",
    "# print(\"Average Accuracy:\", np.mean(accuracies))\n",
    "\n",
    "\n",
    "\n",
    "###for dlbcl data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('DLBCL-2.csv')\n",
    "\n",
    "# Split features and target variable\n",
    "X = df.drop(columns=['target'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Perform one-hot encoding on the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the random forest classifier\n",
    "n_trees = 100\n",
    "random_forest = RandomForestClassifier(n_estimators=n_trees)\n",
    "\n",
    "# Fit the classifier\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = random_forest.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "\n",
    "X = df.iloc[:, 1:].values\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def initialize_centroids(X, k):\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = np.zeros((k, n_features))\n",
    "    for i in range(k):\n",
    "        centroid = X[np.random.choice(range(n_samples))]\n",
    "        centroids[i] = centroid\n",
    "    return centroids\n",
    "\n",
    "def assign_clusters(X, centroids):\n",
    "    clusters = np.zeros(len(X))\n",
    "    for i, sample in enumerate(X):\n",
    "        distances = [euclidean_distance(sample, centroid) for centroid in centroids]\n",
    "        cluster = np.argmin(distances)\n",
    "        clusters[i] = cluster\n",
    "    return clusters\n",
    "\n",
    "def update_centroids(X, clusters, k):\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = np.zeros((k, n_features))\n",
    "    for i in range(k):\n",
    "        cluster_samples = X[clusters == i]\n",
    "        centroid = np.mean(cluster_samples, axis=0)\n",
    "        centroids[i] = centroid\n",
    "    return centroids\n",
    "\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    for _ in range(max_iters):\n",
    "        clusters = assign_clusters(X, centroids)\n",
    "        prev_centroids = centroids\n",
    "        centroids = update_centroids(X, clusters, k)\n",
    "        if np.all(prev_centroids == centroids):\n",
    "            break\n",
    "    return clusters, centroids\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    clusters, centroids = kmeans(X, i)\n",
    "    wcss.append(np.sum([euclidean_distance(X[j], centroids[int(clusters[j])]) ** 2 for j in range(len(X))]))\n",
    "\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "clusters, centroids = kmeans(X, 2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(2):\n",
    "    plt.scatter(X[clusters == i][:, 0], X[clusters == i][:, 1], label=f'Cluster {i}', alpha=0.6, s=100)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', label='Centroids')\n",
    "plt.xlabel('CGPA')\n",
    "plt.ylabel('IQ')\n",
    "plt.title('K-means Clustering')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "clustered_data = defaultdict(list)\n",
    "\n",
    "for i, cluster_label in enumerate(clusters):\n",
    "    clustered_data[int(cluster_label)].append(X[i])\n",
    "\n",
    "for cluster_label, cluster_points in clustered_data.items():\n",
    "    print(f\"Cluster {cluster_label}:\")\n",
    "    for point in cluster_points:\n",
    "        print(point)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\n",
    "\n",
    "T = np.array([1, -1, -1, -1])\n",
    "\n",
    "w = np.array([0, 0])\n",
    "b = 0\n",
    "lr = 1\n",
    "\n",
    "def activation(yin):\n",
    "    if yin > 0:\n",
    "        return 1\n",
    "    elif yin == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        yin = b + np.dot(X[i], w)\n",
    "        y = activation(yin)\n",
    "        if y != T[i]:\n",
    "            w[0] += lr * T[i] * X[i][0]\n",
    "            w[1] += lr * T[i] * X[i][1]\n",
    "            b += lr * T[i]\n",
    "\n",
    "print(\"Final weights:\", w)\n",
    "print(\"Final bias:\", b)\n",
    "\n",
    "\n",
    "def predict(x1, x2):\n",
    "    yin = b + np.dot([x1, x2], w)\n",
    "    return activation(yin)\n",
    "\n",
    "# Test the predict function\n",
    "print(\"Input (1, 1): Predicted Output:\", predict(1, 1))\n",
    "print(\"Input (1, -1): Predicted Output:\", predict(1, -1))\n",
    "print(\"Input (-1, 1): Predicted Output:\", predict(-1, 1))\n",
    "print(\"Input (-1, -1): Predicted Output:\", predict(-1, -1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
